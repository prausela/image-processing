{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"martingrzzler/kanjis2radicals\")\n",
    "\"\"\" Item in ds['train']\n",
    " Dataset({\n",
    "    features: ['kanji_image', 'meta'],\n",
    "    num_rows: 2027\n",
    "})\n",
    "\n",
    "{'kanji_image': Image(mode=None, decode=True, id=None), \n",
    " 'meta': {'id': Value(dtype='int32', id=None), \n",
    "          'characters': Value(dtype='string', id=None), \n",
    "          'meanings': Value(dtype='string', id=None), \n",
    "          'radicals': Sequence(feature={'characters': Value(dtype='string', id=None), \n",
    "                                        'id': Value(dtype='int32', id=None), \n",
    "                                        'slug': Value(dtype='string', id=None)\n",
    "                                        }, length=-1, id=None)\n",
    "         }\n",
    "}\n",
    "\"\"\"\n",
    "# Split the dataset into train and test (80-20 split by default)\n",
    "ds_split = ds['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Access the train and test subsets\n",
    "ds_train = ds_split['train']\n",
    "ds_test = ds_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique radicals: 478\n",
      "Sorted unique radicals: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769, 8770, 8771, 8772, 8773, 8774, 8775, 8776, 8777, 8778, 8779, 8780, 8781, 8782, 8783, 8784, 8785, 8787, 8788, 8790, 8792, 8793, 8794, 8796, 8797, 8798, 8799, 8819, 8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829, 8830, 8831, 8832]\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique radicals\n",
    "unique_radicals = set()\n",
    "for element in ds['train']:\n",
    "    radical_ids = element['meta']['radicals']['id']\n",
    "    unique_radicals.update(radical_ids)\n",
    "\n",
    "# Sort the unique radicals\n",
    "sorted_unique_radicals = sorted(unique_radicals)\n",
    "\n",
    "# Count total unique radicals\n",
    "total_unique_radicals = len(sorted_unique_radicals)\n",
    "\n",
    "print(f\"Total unique radicals: {total_unique_radicals}\")\n",
    "print(f\"Sorted unique radicals: {sorted_unique_radicals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vector(ds):\n",
    "    # Crear un mapeo de radicales únicos a índices\n",
    "    radical_to_index = {radical: idx for idx, radical in enumerate(unique_radicals)}\n",
    "\n",
    "    # Transformar los identificadores de radicales en vectores binarios\n",
    "    def generate_label_vector(radical_ids, total_radicals):\n",
    "        label_vector = [0] * total_radicals\n",
    "        for radical_id in radical_ids:\n",
    "            if radical_id in radical_to_index:\n",
    "                label_vector[radical_to_index[radical_id]] = 1\n",
    "        return label_vector\n",
    "\n",
    "    # Generar labels para el dataset de entrenamiento\n",
    "    total_unique_radicals = len(unique_radicals)\n",
    "    labels = []\n",
    "\n",
    "    for element in ds:\n",
    "        radical_ids = element['meta']['radicals']['id']\n",
    "        label_vector = generate_label_vector(radical_ids, total_unique_radicals)\n",
    "        labels.append(label_vector)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = label_vector(ds_train)\n",
    "test_labels = label_vector(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),         # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for RGB\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(ds_image, ds_label):\n",
    "    # Define a transformation pipeline for the images\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for i, _ in enumerate(ds_image):\n",
    "        # Example image path and label (replace with actual paths)\n",
    "        radical_ids = ds_label[i]\n",
    "        \n",
    "        # Load and transform the image\n",
    "        image = ds_image[i]['kanji_image'].convert(\"RGB\")\n",
    "        image_tensor = image_transform(image)\n",
    "        \n",
    "        # Convert radical IDs to tensor\n",
    "        label_tensor = torch.tensor(radical_ids, dtype=torch.long)\n",
    "        \n",
    "        # Append to batch lists\n",
    "        batch_images.append(image_tensor)\n",
    "        batch_labels.append(label_tensor)\n",
    "\n",
    "    # Stack into a batch (list of tensors to tensor batch)\n",
    "    batch_images = torch.stack(batch_images)\n",
    "    batch_labels = torch.nn.utils.rnn.pad_sequence(batch_labels, batch_first=True, padding_value=-1)\n",
    "\n",
    "    print(f\"Batch Images Shape: {batch_images.shape}\")  # [batch_size, channels, height, width]\n",
    "    print(f\"Batch Labels Shape: {batch_labels.shape}\")  # [batch_size, max_seq_len]\n",
    "    return batch_images, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Images Shape: torch.Size([1621, 3, 128, 128])\n",
      "Batch Labels Shape: torch.Size([1621, 478])\n",
      "Batch Images Shape: torch.Size([406, 3, 128, 128])\n",
      "Batch Labels Shape: torch.Size([406, 478])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_img, train_labels = sample_batch(ds_train, train_labels)\n",
    "test_img, test_labels = sample_batch(ds_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collate_fn to pad sequences and stack the images\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack all images in the batch\n",
    "    labels = [item[1] for item in batch]  # Get the labels (radical IDs) for each image\n",
    "    \n",
    "    # Pad the label sequences\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return images, labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanjiRadicalDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list): List of file paths to the Kanji images.\n",
    "            labels (list): List of corresponding radical ID lists for each image.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary with 'image' and 'label' tensors.\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing\n",
    "train_dataset = KanjiRadicalDataset(images=train_img, labels=train_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda batch: collate_fn(batch))\n",
    "\n",
    "test_dataset = KanjiRadicalDataset(images=test_img, labels=test_labels)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=lambda batch: collate_fn(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block_nested(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super(conv_block_nested, self).__init__()\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        output = self.activation(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nested_UNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=1):\n",
    "        super(Nested_UNet, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
    "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
    "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
    "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
    "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
    "\n",
    "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n",
    "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
    "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
    "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
    "\n",
    "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n",
    "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
    "\n",
    "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
    "\n",
    "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(filters[0], out_ch, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0_0 = self.conv0_0(x)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        output = output.view(output.size(0), -1)  # [batch_size, out_ch]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ImageToRadicalsModel(nn.Module):\n",
    "#     def __init__(self, num_radicals):\n",
    "#         super(ImageToRadicalsModel, self).__init__()\n",
    "        \n",
    "#         # Define the convolutional layers to process the image\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "#         self.fc1 = nn.Linear(256 * 16 * 16, 512)  # Adjust based on input size and feature maps\n",
    "        \n",
    "#         # Define an LSTM or GRU for sequence generation\n",
    "#         self.rnn = nn.LSTM(input_size=512, hidden_size=256, batch_first=True)\n",
    "        \n",
    "#         # Output layer\n",
    "#         self.output = nn.Linear(256, num_radicals)  # num_radicals is the size of your output space\n",
    "    \n",
    "#     # Modificación del forward sin secuencias\n",
    "#     def forward(self, x):\n",
    "#         # Pasar por las capas convolucionales\n",
    "#         x = self.conv1(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = torch.relu(x)\n",
    "        \n",
    "#         # Aplanar para la capa totalmente conectada\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc1(x)\n",
    "        \n",
    "#         # Pasar directamente por la capa de salida\n",
    "#         x = self.output(x)\n",
    "#         return x  # Output: [batch_size, num_radicals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 102/102 [1:01:10<00:00, 35.99s/it, Loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.6263906885595882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 102/102 [57:12<00:00, 33.65s/it, Loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.47836516271619234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 102/102 [56:22<00:00, 33.16s/it, Loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.3624286596097198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 102/102 [56:13<00:00, 33.07s/it, Loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 0.2789720743894577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 102/102 [58:41<00:00, 34.53s/it, Loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.21832982333851794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 102/102 [54:17<00:00, 31.93s/it, Loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 0.1740181386178615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 102/102 [53:13<00:00, 31.31s/it, Loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 0.14157142124924005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 102/102 [53:07<00:00, 31.25s/it, Loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 0.11749379906584234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 102/102 [52:42<00:00, 31.01s/it, Loss=0.0918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 0.09928200101735545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 102/102 [53:04<00:00, 31.22s/it, Loss=0.0804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 0.08547876855614138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 102/102 [52:41<00:00, 30.99s/it, Loss=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Average Loss: 0.0747518559150836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 102/102 [52:26<00:00, 30.85s/it, Loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Average Loss: 0.06636176751378704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 102/102 [52:55<00:00, 31.13s/it, Loss=0.0577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Average Loss: 0.05977824363200104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 102/102 [53:26<00:00, 31.44s/it, Loss=0.0514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Average Loss: 0.05443828658876466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 102/102 [52:30<00:00, 30.89s/it, Loss=0.047] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Average Loss: 0.05016086199412159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 102/102 [52:20<00:00, 30.79s/it, Loss=0.0447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Average Loss: 0.04662652894416276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 102/102 [52:47<00:00, 31.05s/it, Loss=0.0404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Average Loss: 0.04372767216580756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 102/102 [51:45<00:00, 30.45s/it, Loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Average Loss: 0.04132895073031678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 102/102 [51:44<00:00, 30.43s/it, Loss=0.0381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Average Loss: 0.039277153162687435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 102/102 [51:31<00:00, 30.31s/it, Loss=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Average Loss: 0.0375945425281922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 102/102 [51:52<00:00, 30.52s/it, Loss=0.0378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Average Loss: 0.036121578485358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 102/102 [52:15<00:00, 30.74s/it, Loss=0.0302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Average Loss: 0.034815666850144956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 102/102 [52:21<00:00, 30.80s/it, Loss=0.0354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Average Loss: 0.03376211423207732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 102/102 [51:47<00:00, 30.47s/it, Loss=0.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Average Loss: 0.032782252366636316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 102/102 [52:12<00:00, 30.71s/it, Loss=0.0357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Average Loss: 0.03196765920695137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 102/102 [52:10<00:00, 30.70s/it, Loss=0.0282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Average Loss: 0.03116477226071498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 102/102 [52:07<00:00, 30.66s/it, Loss=0.0283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Average Loss: 0.030478006447939313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 102/102 [53:19<00:00, 31.37s/it, Loss=0.029] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Average Loss: 0.029865755279566728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 102/102 [51:52<00:00, 30.51s/it, Loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Average Loss: 0.029295084870183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 102/102 [52:02<00:00, 30.62s/it, Loss=0.024] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Average Loss: 0.028733470276290297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 102/102 [52:05<00:00, 30.65s/it, Loss=0.0284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Average Loss: 0.02827517074697158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 102/102 [51:59<00:00, 30.59s/it, Loss=0.025] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Average Loss: 0.027773212111902004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 102/102 [51:29<00:00, 30.29s/it, Loss=0.0292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Average Loss: 0.027344678802525297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 102/102 [51:47<00:00, 30.47s/it, Loss=0.0276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Average Loss: 0.026909109625015772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 102/102 [52:11<00:00, 30.70s/it, Loss=0.0263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Average Loss: 0.02647550732773893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 102/102 [52:12<00:00, 30.72s/it, Loss=0.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Average Loss: 0.026046739854649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 102/102 [52:16<00:00, 30.75s/it, Loss=0.0218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Average Loss: 0.025599406776474973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 102/102 [52:21<00:00, 30.80s/it, Loss=0.0275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Average Loss: 0.02509378928545059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 102/102 [52:11<00:00, 30.70s/it, Loss=0.0271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Average Loss: 0.024520558542480655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 102/102 [52:01<00:00, 30.60s/it, Loss=0.024] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Average Loss: 0.02401739727778762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 102/102 [53:07<00:00, 31.25s/it, Loss=0.0225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Average Loss: 0.0234062440656856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 102/102 [52:08<00:00, 30.67s/it, Loss=0.0268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Average Loss: 0.02292718964756704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 102/102 [52:06<00:00, 30.66s/it, Loss=0.0264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Average Loss: 0.022463129888124326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 102/102 [52:09<00:00, 30.68s/it, Loss=0.0209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Average Loss: 0.021932371599855376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 102/102 [52:03<00:00, 30.62s/it, Loss=0.0211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Average Loss: 0.021492951001752827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 102/102 [52:12<00:00, 30.71s/it, Loss=0.0249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Average Loss: 0.02109036917853005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 102/102 [4:47:21<00:00, 169.04s/it, Loss=0.0242]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Average Loss: 0.020690995067650198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 102/102 [53:03<00:00, 31.21s/it, Loss=0.0281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Average Loss: 0.020335925487326642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 102/102 [56:05<00:00, 32.99s/it, Loss=0.0231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Average Loss: 0.019842500579269493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 102/102 [52:26<00:00, 30.85s/it, Loss=0.0213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Average Loss: 0.01948562264442444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Importar tqdm para la barra de progreso\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "num_radicals = 478  # Total unique radicals in the dataset\n",
    "model = Nested_UNet(3, num_radicals)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Multi-label classification\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(50):  # Number of epochs\n",
    "    model.train()\n",
    "    epoch_loss = 0  # Para calcular la pérdida acumulada por epoch\n",
    "\n",
    "    # Barra de progreso para batches\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}\") as pbar:\n",
    "        for batch in pbar:\n",
    "            images, targets = batch  # Images: [batch_size, 3, 128, 128], Targets: [batch_size, num_radicals]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images)  # Output: [batch_size, num_radicals]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, targets.float())  # Convert targets to float for BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})  # Actualiza la barra de progreso con la pérdida actual\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {epoch_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [04:39<00:00, 10.73s/it, Loss=0.0216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.024856121709140446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0  # To accumulate the loss\n",
    "all_predictions = []  # To store all predictions\n",
    "all_targets = []  # To store all true labels\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    with tqdm(test_loader, desc=\"Testing\") as pbar:  # Use tqdm for progress\n",
    "        for batch in pbar:\n",
    "            images, targets = batch  # Images: [batch_size, 3, 128, 128], Targets: [batch_size, num_radicals]\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images)  # Output: [batch_size, num_radicals]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, targets.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Store predictions and targets for further analysis\n",
    "            all_predictions.append(torch.sigmoid(predictions))  # Apply sigmoid for probabilities\n",
    "            all_targets.append(targets)\n",
    "\n",
    "            # Update tqdm with loss information\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Combine all predictions and targets\n",
    "all_predictions = torch.cat(all_predictions, dim=0).cpu()\n",
    "all_targets = torch.cat(all_targets, dim=0).cpu()\n",
    "\n",
    "# Example: Analyze results (optional)\n",
    "# Convert predictions to binary (threshold=0.5)\n",
    "binary_predictions = (all_predictions > 0.5).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [05:33<00:00, 12.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Configuración\n",
    "model.eval()  # Ponemos el modelo en modo de evaluación\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Transformaciones para la imagen de entrada (ajusta según tu entrenamiento)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Asegurarse de que el tamaño coincida con el de entrenamiento\n",
    "    transforms.ToTensor(),  # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizar (ajusta según tu dataset)\n",
    "])\n",
    "\n",
    "predicted_labels = []\n",
    "with tqdm(test_loader, desc=\"Testing\") as pbar:  # Use tqdm for progress\n",
    "    for batch in pbar:\n",
    "        images, targets = batch  # Images: [batch_size, 3, 128, 128], Targets: [batch_size, num_radicals]\n",
    "\n",
    "        # Enviar la imagen al dispositivo\n",
    "        image_tensor = images.to(device)\n",
    "\n",
    "        # Inferencia\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_tensor)  # Salida: [1, num_radicals]\n",
    "            predictions = torch.sigmoid(predictions)  # Aplicar sigmoide para convertir a probabilidades\n",
    "\n",
    "        # Procesar las salidas\n",
    "        threshold = 0.5  # Define un umbral para clasificar\n",
    "        predicted_labels.append((predictions > threshold).int())  # [1, num_radicals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(all_targets[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
